{"cells":[{"cell_type":"markdown","source":["# Analyze OneLake Storage Usage in a Fabric Workspace\n","\n","### Overview\n","* Recursively traverse folders and files in a given Workspace or OneLake path. \n","* Return a list of all files within  the path broken out by path segment and enrich the data with relevant attributes, such as:\n","    - File Extension\n","    - Modification Time\n","    - Size\n","    - Etc.\n","* Provide a few basic summaries of the collected data: total for the entire path, breakdown by file extension, by Fabric Item, etc.\n","\n","### Usage Instructions\n","This notebook is intended to be executed in a Microsoft Fabric workspace. The person executing this notebook must have access to workspaces and items whose storage usage is being analyzed.\n","1. Download and import this Spark Notebook into a Fabric Workspace. The workspace must be hosted on a Fabric (or Premium) Capacity. \n","1. Specify values for the *workspace_name* and *folder_path* parameters in a code cell below\n","1. Run all code cells\n","1. Review the results\n","1. If desired, you may extend this notebook to store the list of files to a repository of your choice (e.g., a Fabric Lakehouse) to maintain a historical record of your storage usage.\n","\n","### Limitations\n","This notebook will attempt to use \"user-friendly\" workspace names and item names. However, if user-friendly names contain characters that cannot be safely used in URLs, item identifiers will be replaced by globally-unique identifiers GUIDs. Also note that Fabric Item Types cannot be properly detected for items identified using GUIDs.\n","\n","### Important Notice!\n","<mark>Data volumes returned by this notebook **will not match data volumes reported by the Fabric Capacity Metrics App** due to substantial differences in methodologies for calculating data volumes. For example, this notebook will include files available via shortcuts, even if these files do not physically reside in this OneLake path (and are merely logically referenced within this path). Similarly, this notebook may include storage that is considered non-billable (e.g., mirroring storage). Furthermore, this notebook will not include files that have been deleted, but are being temporarily retained to provide protection from accidental deletion (soft-deleted data in OneLake is billed at the same rate as active data). \n","Therefore, while this notebook can serve as a helpful tool in analyzing your storage, *it should not be used to estimate billable storage.*</mark>\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a784197-c0b0-4b4a-97de-edd75826c314"},{"cell_type":"code","source":["#Specify Fabric workspace name (or leave blank to collect data for the current workspace)\n","workspace_name = \"\"\n","\n","#Specify folder path within the workspace (or leave blank to collect data for all items in the workspace)\n","folder_path = \"\" "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5aa68225-9d23-448e-b315-7d6d3e1a4497"},{"cell_type":"code","source":["#Import relevant libraries\n","import sempy.fabric as fabric\n","import urllib.parse\n","\n","#Resolve workspace name\n","workspace_name = fabric.resolve_workspace_name(None if workspace_name == \"\" else workspace_name)\n","#URL encode workspace name\n","workspace_name_quoted = urllib.parse.quote_plus(workspace_name)\n","\n","#If workspace name is the same as it's encoded name, use the workspace name, otherwise use the GUID:\n","if workspace_name == workspace_name_quoted:\n","    workspace_id = workspace_name\n","else:\n","    workspace_id = fabric.resolve_workspace_id(workspace_name)\n","\n","path = f\"abfss://\" + str.replace(workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + folder_path + '/', \"//\",\"/\")\n","\n","print(\"Processing the following path: \" + path)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"4b385285-5d98-41ed-b232-8b59b21e4bc4"},{"cell_type":"code","source":["import pandas as pd\n","from notebookutils import mssparkutils\n","from typing import List\n","from concurrent.futures import ThreadPoolExecutor\n","import concurrent.futures\n","\n","def get_descendants(path):\n","    # Recursively get files in a directory path\n","    new_paths = mssparkutils.fs.ls(path)\n","    files = []\n","\n","    def get_children(path):\n","        return mssparkutils.fs.ls(path.path)\n","\n","    while len(new_paths) > 0:\n","        current_paths = new_paths\n","        new_paths = []\n","\n","        #Parallelize retrieval of child items from each path\n","        with ThreadPoolExecutor(max_workers=16) as executor:\n","            results = executor.map(get_children, current_paths)\n","            children = sum(results, [])\n","            for child in children:\n","                if child.isDir:\n","                    new_paths.append(child)\n","                else:\n","                    files.append(child)\n","    return files\n","\n","def file_list_to_dataframe(files):\n","    # Convert file list into a Pandas data frame and enrich the data to facilitate analysis\n","    schema = ['path','name','size', 'modifyTime']\n","    df = pd.DataFrame([[getattr(file, attribute) for attribute in schema] for file in files], columns = schema).sort_values('path')\n","    loc_0 = df.columns.get_loc('name')\n","    df_split = df['name'].str.split(pat='.', expand=True, n=1).add_prefix('name_')\n","    df = pd.concat([df.iloc[:, :loc_0], df_split, df.iloc[:, loc_0:]], axis=1)\n","    df = df.drop(columns=['name'])\n","    df = df.rename(columns={'name_1': 'FileExtension'})\n","    df = df.drop(columns=['name_0'])\n","    df['path'] = df['path'].str.replace(\"abfss://\", \"\", case=False, regex=False)\n","    loc_0 = df.columns.get_loc('path')\n","    df_split = df['path'].str.split(pat='/', expand=True).add_prefix('Path_')\n","    df = pd.concat([df.iloc[:, :loc_0], df_split, df.iloc[:, loc_0:]], axis=1)\n","    df['SizeInMB'] = df['size']/(pow(2, 20))\n","    df['modifyTime'] = pd.to_datetime(df['modifyTime'], unit = 'ms')\n","    df = df.drop(columns=['size'])\n","    df = df.rename(columns={'path': 'FullPath'})\n","    df['Depth'] = df['FullPath'].str.count('/')\n","    df['ItemType'] = df['Path_1'].str.rsplit(pat='.', expand=False, n=1).str[-1]\n","    return df\n","\n","#Get all files from path\n","df = file_list_to_dataframe(get_descendants(path))   "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db6267d8-2873-45dc-80b2-015c64ec6b3b"},{"cell_type":"code","source":["#Summarize data for the entire path\n","dfSummary = df.groupby(['Path_0']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"15d31318-df95-4b38-b365-ad014874a747"},{"cell_type":"code","source":["#Summarize data by Fabric Item Type\n","#Note: item types cannot be properly detected for items identified by GUIDs (rather than user-friendly names) in this scenario, item GUIDs will appear in the ItemType column.\n","dfSummary = df.groupby(['ItemType']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"35b81fe8-1c07-48be-80d9-a8e1dc5234dc"},{"cell_type":"code","source":["#Summarize data by Fabric Item\n","dfSummary = df.groupby(['Path_1']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dd292111-5071-49dd-80f9-535786f0f5e5"},{"cell_type":"code","source":["#Summarize data by File Extension\n","dfSummary = df.groupby(['FileExtension']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c7984a06-657c-4a46-a879-53f166f9c376"},{"cell_type":"code","source":["#Summarize data for the first 3 path segments\n","dfSummary = df.groupby(['Path_1', 'Path_2', 'Path_3']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"230df7bf-89b7-467c-b8ec-cffde845220b"},{"cell_type":"code","source":["#Return the full list of files\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dd99a63e-8f64-4b63-b75e-02b888a18e69"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}