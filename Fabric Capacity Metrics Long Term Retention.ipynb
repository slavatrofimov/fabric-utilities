{"cells":[{"cell_type":"markdown","source":["# Fabric Capacity Metrics Long Term Retention\n\n## Background and Purpose\nThe Fabric Capacity Metrics App enables Microsoft Fabric Capacity (and Premium Capacity) administrators to track utilization of Fabric Capacities over the most recent 14-day period.\n\nThere may be scenarios where longer-term history of capacity utilization is needed to understand long term trends, identify seasonal workload patterns, and facilitate future capacity planning. \n\nThis notebook can be used as a solution accelerator for importing summaries of Fabric Capacity metrics from the Fabric Capacity Metrics App for long term storage and analysis. The data is imported at the granularity of one record per item per operation type per hour. The data is stored in a Fabric Lakehouse for long-term storage and analysis.\n\n## Warning!\n<mark>This solution is not approved and not supported by Microsoft! The structure and content of the Fabric Capacity Metrics App that serves as the data source for this solution may change at any time without notice, rendering this solution inoperable. Use at your own risk.</mark>\n\n## Installation and Usage Instructions\n1. You must be an administrator of one or more Fabric and/or Premium Capacities.\n1. Install the [Fabric Capacity Metrics App](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-install?tabs=1st)\n1. Download and [import this Spark Notebook into a Fabric Workspace](https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#import-existing-notebooks). The workspace must be hosted on a Fabric (or Premium) Capacity. This workspace can (and should) be different from the workspace where the Fabric Capacity Metrics App resides -- this will ensure that your Lakehouse with historical data remains unaffected if you choose to delete and reinstall the Fabric Capacity Metrics App.\n1. [Connect a new or existing Fabric Lakehouse to your notebook](https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#connect-lakehouses-and-notebooks).\n1. If necessary, modify the parameter values in this notebook, such as the name of the Workspace and Semantic Model associated with the Fabric Capacity Metrics App that will serve as the source of your data.\n1. Execute the notebook for the first time, which will create the following tables in your Lakehouse: \n    - **hourly_capacity_metrics** stores summaries of capacity operations at the granularity of one record per hour per item\n    - **items** stores a catalog of items that can provide context for metrics stored in the hourly_capacity_metrics table\n    - **processing_log** a helper table that will store one record for each data load operation. This table facilitates efficient incremental retrieval of historical metrics.\n1. Finally, schedule the Notebook to run on a recurring schedule (such as daily). Note: to avoid gaps in historical data, the notebook must be executed at least once per 14-day period (preferably at least weekly).\n\n## Usage Notes\n- This solution imports summaries of data at the granularity of one record per hour per item (rather than a detailed log of individual operations). The granularity of this data may or may not be sufficient for your needs.\n- This solution uses the identity context of the person who is running this notebook (or who owns the notebook in a scheduled/unattended execution scenario)\n- This solution will retrieve data only from capacities in which the user is an administrator\n- By default, this solution will merge all data into delta tables in the connected Lakehouse and will also store raw data files in parquet format in the Files area of the Lakehouse. If desired, you may disable the storage of raw parquet files by setting the *store_raw_data* parameter to *False*.\n- To improve efficiency, this solution filters out summary records where CU consumption, total duration and total operation counts are all 0.\n- To analyze the data, you may use Spark-based notebooks, query the data using the built-in SQL Analytics Endpoints, create a semantic model and corresponding reports, or use any other suitable analytical tools.\n\n### Acknowledgements\nThe design of this notebook has benefited from the work of [Kristian Bubalo](https://pbi-guy.com/2024/04/24/how-to-extract-data-from-the-fabric-metrics-app-part-2/). Please review his blog posts and sample solutions for additional perspectives on extracting and storing historical capacity metrics."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f9dfd03-2020-419d-9aac-4467cde4f3f2"},{"cell_type":"code","source":["#Import required libraries\n","import sempy.fabric as fabric\n","from datetime import date, datetime, timedelta\n","from pyspark.sql.functions import current_timestamp, date_format\n","from pyspark.sql import functions as F\n","\n","#Set relevant spark configuration settings\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ae25e3e-e694-4108-8fce-12f2205837dd"},{"cell_type":"code","source":["#Define parameters\n","source_workspace = 'Microsoft Fabric Capacity Metrics'\n","source_dataset = 'Fabric Capacity Metrics'\n","\n","#Specify whether to store raw data in the Files area of the Lakehouse (default is true)\n","store_raw_data = True\n","path_prefix = 'Files/Raw Capacity Metrics History/'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"eaa2e58b-58e2-441a-8c63-cd451a58b41d"},{"cell_type":"code","source":["#Initialize the processing log table (if needed)\n","sql_statement = \"\"\"\n","    -- Create a table to store the processing log (if it does not already exist)\n","    CREATE TABLE IF NOT EXISTS processing_log AS \n","    SELECT current_timestamp() AS Timestamp, CAST('2000-01-01' AS timestamp) AS WatermarkStart, CAST('2000-01-01' AS timestamp) AS WatermarkEnd;\n","    \"\"\"\n","spark.sql(sql_statement)\n","\n","#Get the latest ending watermark from the processing_log\n","sql_statement = \" SELECT MAX(WatermarkEnd) AS Watermark FROM processing_log\"\n","df = spark.sql(sql_statement)\n","watermark_start = df.first()['Watermark']\n","\n","#Set new watermark to a time point 1-day ago (to allow for potential to reprocess late-arriving records)\n","watermark_end = datetime.now() - timedelta(hours=25)\n","\n","current_timestamp = datetime.now()\n","\n","#Add a record to processing log to indicate completion of a data loading operation\n","sql_statement = \"\"\"INSERT INTO processing_log (Timestamp, WatermarkStart) \n","SELECT '\"\"\" + format(current_timestamp) + \"', '\" + format(watermark_start) + \"'\"\n","spark.sql(sql_statement)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb567199-4606-480b-b602-dcd177f9eac5"},{"cell_type":"markdown","source":["## Extract and Store Item Catalog"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea7e94b5-b451-47da-88e9-5f16611e7faf"},{"cell_type":"code","source":["#Define a function to extract the catalog of items from the Fabric Capacity Metrics App semantic model\n","def extractItems(store_raw_data, current_timestamp, path_prefix):\n","    dax_query = \"\"\"\n","    EVALUATE\n","        SUMMARIZECOLUMNS (\n","            Items[Billable type],\n","            Items[capacityId],\n","            Items[dcount_Identity],\n","            Items[IsVirtualArtifactName],\n","            Items[IsVirtualArtifactStatus],\n","            Items[IsVirtualWorkspaceName],\n","            Items[IsVirtualWorkspaceStatus],\n","            Items[ItemId],\n","            Items[ItemKey],\n","            Items[ItemKind],\n","            Items[ItemName],\n","            Items[Timestamp],\n","            Items[UniqueKey],\n","            Items[WorkspaceId],\n","            Items[WorkspaceName]\n","        )\"\"\"\n","\n","    df_items = fabric.evaluate_dax(\n","        workspace = source_workspace,\n","        dataset = source_dataset,\n","        dax_string = dax_query,\n","        verbose = 1\n","        )\n","\n","    if not df_items.empty: \n","        #Convert Fabric DataFrames into Spark DataFrames\n","        dfs_items = spark.createDataFrame(df_items)\n","        #Clean up column names\n","        dfs_items = dfs_items.select([F.col(x).alias(x.replace('Items[','').replace(' ', '_').replace('[', '').replace(']', '').replace('(', '').replace(')','')) for x in dfs_items.columns])\n","        #Create a temporary view on top of the data frame\n","        dfs_items.createOrReplaceTempView('current_items')\n","\n","        sql_statement = \"\"\"\n","            -- Create a table to store items catalog (if it does not already exist)\n","            CREATE TABLE IF NOT EXISTS items AS \n","            SELECT * FROM current_items LIMIT 0; \n","            \"\"\"\n","        spark.sql(sql_statement)\n","\n","        sql_statement = \"\"\"\n","            -- Merge currenly retrieved items into the permanent items table\n","            MERGE INTO items AS target\n","            USING current_items AS source\n","            ON source.UniqueKey = target.UniqueKey\n","            WHEN MATCHED THEN\n","            UPDATE SET *\n","            WHEN NOT MATCHED THEN\n","            INSERT *;\n","            \"\"\"\n","        spark.sql(sql_statement)\n","\n","        if store_raw_data:\n","            #Set folder path\n","            path = path_prefix + 'Items/' + current_timestamp.strftime(\"%Y/%m/%d\") + '/' + current_timestamp.strftime(\"%H-%M-%S\")\n","            #Save DataFrames to OneLake\n","            dfs_items.write.mode(\"overwrite\").format(\"parquet\").save(path)\n","\n","#Execute the function to extract Items\n","extractItems(store_raw_data, current_timestamp, path_prefix)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8763dd92-23ee-4de9-a57c-eaa6be9f3fcb"},{"cell_type":"code","source":["%%sql\n","SELECT * FROM items \n","ORDER BY ItemKey, capacityid"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"0dfe68d2-5a3e-4a89-99b9-5748dbe9f5e5"},{"cell_type":"markdown","source":["## Extract and Store Hourly Capacity Metrics"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cfbb8ed9-9762-4fb8-9932-06d36f75d2d0"},{"cell_type":"code","source":["#Extract the hourly metrics by item by operation from the Fabric Capacity Metrics App semantic model\n","#The data is pre-filtered from start watermark forward\n","def extractMetrics(capacityId, watermark_start, store_raw_data, current_timestamp, path_prefix):\n","\n","    #Parse date parts for \n","    wm_year = str(watermark_start.year)\n","    wm_month = str(watermark_start.month)\n","    wm_day = str(watermark_start.day)\n","\n","    dax_query = \"\"\"\n","    DEFINE\n","            MPARAMETER 'CapacityID' = \"\"\" + '\"' + capacityId + '\"' + \"\"\"\n","            VAR varFilter_Capacity =\n","                TREATAS ( { \"\"\" + '\"' + capacityId + '\"' + \"\"\" }, 'Capacities'[capacityId] )\n","    EVALUATE\n","        SUMMARIZECOLUMNS (\n","            'MetricsByItemandOperationandHour'[UniqueKey],\n","            'MetricsByItemandOperationandHour'[ItemId],\n","            'MetricsByItemandOperationandHour'[WorkspaceId],\n","            'MetricsByItemandOperationandHour'[PremiumCapacityId],\n","            'MetricsByItemandOperationandHour'[DateTime],\n","            'MetricsByItemandOperationandHour'[OperationName],\n","            'MetricsByItemandOperationandHour'[sum_CU],\n","            'MetricsByItemandOperationandHour'[sum_duration],\n","            'MetricsByItemandOperationandHour'[Throttling (min)],\n","            'MetricsByItemandOperationandHour'[count_users],\n","            'MetricsByItemandOperationandHour'[count_successful_operations],\n","            'MetricsByItemandOperationandHour'[count_rejected_operations],\n","            'MetricsByItemandOperationandHour'[count_operations],\n","            'MetricsByItemandOperationandHour'[count_Invalid_operations],\n","            'MetricsByItemandOperationandHour'[count_InProgress_operations],\n","            'MetricsByItemandOperationandHour'[count_failure_operations],\n","            'MetricsByItemandOperationandHour'[count_cancelled_operations],\n","            'MetricsByItemandOperationandHour'[avg_DurationMS],\n","            'MetricsByItemandOperationandHour'[percentile_DurationMs_50],\n","            'MetricsByItemandOperationandHour'[percentile_DurationMs_90],\n","            FILTER ('MetricsByItemandOperationandHour', \n","                'MetricsByItemandOperationandHour'[DateTime] >= DATE(\"\"\" + wm_year + \", \" + wm_month + \", \" + wm_day + \"\"\")\n","                && ('MetricsByItemandOperationandHour'[sum_CU] > VALUE(0) \n","                    || 'MetricsByItemandOperationandHour'[sum_duration] > VALUE(0)\n","                    || 'MetricsByItemandOperationandHour'[count_operations] > VALUE(0))\n","                ),\n","            varFilter_Capacity \n","            )\n","        \"\"\"\n","\n","    df_metrics = fabric.evaluate_dax(\n","        workspace = source_workspace,\n","        dataset = source_dataset,\n","        dax_string = dax_query,\n","        verbose = 1\n","        )\n","\n","    if not df_metrics.empty:        \n","        #Convert Fabric DataFrames into Spark DataFrames\n","        dfs_metrics = spark.createDataFrame(df_metrics)\n","        #Clean up column names\n","        dfs_metrics = dfs_metrics.select([F.col(x).alias(x.replace(' ', '_').replace('[', '').replace(']', '').replace('(', '').replace(')','').replace('MetricsByItemandOperationandHour','')) for x in dfs_metrics.columns])\n","        #Create a temporary view on top of the data frame\n","        dfs_metrics.createOrReplaceTempView('current_metrics')\n","\n","        sql_statement_create = \"\"\"\n","            -- Create a table to store hourly capacity metrics (if it does not already exist)\n","            CREATE TABLE IF NOT EXISTS hourly_capacity_metrics AS \n","            SELECT * FROM current_metrics LIMIT 0;  \n","            \"\"\"\n","        spark.sql(sql_statement_create)\n","\n","        sql_statement_load = \"\"\"\n","            -- Merge currenly retrieved metrics into the permanent table\n","            MERGE INTO hourly_capacity_metrics AS target\n","            USING current_metrics AS source\n","            ON source.UniqueKey = target.UniqueKey\n","                AND source.DateTime = target.DateTime\n","                AND source.OperationName = target.OperationName\n","            WHEN MATCHED THEN\n","            UPDATE SET *\n","            WHEN NOT MATCHED THEN\n","            INSERT *;\n","            \"\"\"\n","        spark.sql(sql_statement_load)\n","\n","        if store_raw_data:\n","            #Set folder path\n","            path = 'Hourly Metrics/' + current_timestamp.strftime(\"%Y/%m/%d\") + '/' \\\n","                + capacityId + '-' + current_timestamp.strftime(\"%H-%M-%S\")\n","            #Save DataFrames to OneLake\n","            dfs_metrics.write.mode(\"overwrite\").format(\"parquet\").save(path_prefix + path)\n","\n","\n","#Get a list of available capacities (excluding the logical Premium Per User - PPU capacity)\n","df_capacities = fabric.list_capacities()\n","df_capacities = df_capacities[df_capacities['Sku'] != 'PP3']\n","\n","#Execute functions for extracting items and metrics for each capacity\n","for capacity in df_capacities.Id:\n","    print('+ Started processing metrics for capacity ' + capacity)\n","    extractMetrics(capacity, watermark_start, store_raw_data, current_timestamp, path_prefix)\n","    print(' - Finished processing metrics for capacity ' + capacity)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"89c680a5-bb63-4c69-8238-0bf0c408da63"},{"cell_type":"markdown","source":["## Log Completion"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"405c2251-b803-4f9b-a159-57382ac90520"},{"cell_type":"code","source":["#Update processing log to indicate completion of a data loading operation\n","sql_statement = \"\"\"\n","UPDATE processing_log \n","SET WatermarkEnd = '\"\"\" + format(watermark_end) + \"\"\"'\n","WHERE Timestamp = '\"\"\" + format(current_timestamp) + \"' AND WatermarkStart = '\" + format(watermark_start) + \"'\"\n","spark.sql(sql_statement)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e43b0b35-1089-405e-b375-3e1ff570fe4e"},{"cell_type":"markdown","source":["## Analyze Results\n","\n","After importing the data, please use your prefered tools to analyze the results. For example, the following Spark SQL query illustrates the top 100 items based on total Capacity Unit consumption.\n","\n","```\n","SELECT I.capacityId, I.WorkspaceName, I.ItemKind, I.ItemName, ROUND(SUM(M.sum_CU),2) AS CapacityUnitSecondsConsumed\n","FROM hourly_capacity_metrics M\n","    LEFT JOIN items I \n","        ON M.UniqueKey = I.UniqueKey\n","GROUP BY I.capacityId, I.WorkspaceName, I.ItemKind, I.ItemName\n","ORDER BY CapacityUnitSecondsConsumed DESC\n","LIMIT 100\n","```"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"397c3992-044a-4a59-81ee-658fa17ebc06"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"a1e2e93c-241a-43e8-bd76-b40469f43c1a","default_lakehouse_name":"LakehouseA","default_lakehouse_workspace_id":"6cdcf164-805a-49f4-897d-196087cef1ef"}}},"nbformat":4,"nbformat_minor":5}