{"cells":[{"cell_type":"markdown","source":["# Fabric Capacity Metrics Long Term Retention\n\n## Background and Purpose\nThe Fabric Capacity Metrics App enables Microsoft Fabric Capacity (and Premium Capacity) administrators to track utilization of Fabric Capacities over the most recent 14-day period.\n\nThere may be scenarios where longer-term history of capacity utilization is needed to understand long term trends, identify seasonal workload patterns, and facilitate future capacity planning. \n\nThis notebook can be used as a solution accelerator for importing summaries of Fabric Capacity metrics from the Fabric Capacity Metrics App for long term storage and analysis. The data is imported at the granularity of one record per item per operation type per hour. The data is stored in a Fabric Lakehouse for long-term storage and analysis.\n\n## Warning!\n<mark>This solution is not approved and not supported by Microsoft. The structure and content of the Fabric Capacity Metrics App that serves as the data source for this solution may change without notice, rendering this solution inoperable. Use at your own risk.</mark>\n\n## Installation and Usage Instructions\n1. You must be an administrator of one or more Fabric and/or Premium Capacities.\n1. Install the [Fabric Capacity Metrics App](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-install?tabs=1st)\n1. Download and [import this Spark Notebook into a Fabric Workspace](https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#import-existing-notebooks). The workspace must be hosted on a Fabric Capacity.\n1. [Connect a new or existing Fabric Lakehouse to your notebook](https://learn.microsoft.com/en-us/fabric/data-engineering/how-to-use-notebook#connect-lakehouses-and-notebooks).\n1. If necessary, modify the parameter values in this notebook, such as the name of the Workspace and Semantic Model associated with the Fabric Capacity Metrics App that will serve as the source of your data.\n1. Execute the notebook for the first time, which will create the following tables in your Lakehouse: \n    - **hourly_capacity_metrics** stores summaries of capacity operations at the granularity of one record per hour per item\n    - **items** stores a catalog of items that can provide context for metrics stored in the hourly_capacity_metrics table\n    - **processing_log** a helper table that will store one record for each successfully completed data load operation. This table facilitiates efficient incremental retrieval of historical metrics.\n1. Finally, schedule the Notebook to run on a recurring schedule (such as daily). Note: to avoid gaps in historical data, the notebook must be executed at least once per 14-day period (preferably at least weekly).\n\n## Usage Notes\n- This solution imports summaries of data (rather than a detailed log of individual operations). The granularity of this data may or may not be sufficient for your needs.\n- This solution uses the identity context of the person who is running this notebook (or who owns the notebook in a scheduled/unattended execution scenario)\n- This solution will retrieve data only from capacities in which the user is an administrator\n- By default, this solution will merge all data into delta tables in the connected Lakehouse and will also store raw data files in parquet format in the Files area of the Lakehouse. If desired, you may disable the storage of raw parquet files by setting the *store_raw_data* parameter to *False*.\n- To improve efficiency, this solution filters out summary records where CU consumption, total duration and total operation counts are all 0.\n- To analyze the data, you may use Spark-based notebooks, query the data using the built-in SQL Analytics Endpoints, create a semantic model and corresponding reports, or use any other suitable analytical tools."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f9dfd03-2020-419d-9aac-4467cde4f3f2"},{"cell_type":"code","source":["#Import required libraries\n","import pandas as pd\n","import sempy.fabric as fabric\n","from datetime import date, datetime, timedelta\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampNTZType\n","from pyspark.sql.functions import current_timestamp, input_file_name, date_format\n","from pyspark.sql import functions as F\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n","spark.conf.set(\"spark.sql.catalog.pbi\", \"com.microsoft.azure.synapse.ml.powerbi.PowerBICatalog\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"13d3a19c-ed66-4053-a1b7-167258d4a7eb","normalized_state":"finished","queued_time":"2024-11-08T20:54:06.870675Z","session_start_time":"2024-11-08T20:54:07.8578028Z","execution_start_time":"2024-11-08T20:54:19.4604358Z","execution_finish_time":"2024-11-08T20:54:32.2478916Z","parent_msg_id":"e9e61369-55ea-451c-8acc-5e60c9820774"},"text/plain":"StatementMeta(, 13d3a19c-ed66-4053-a1b7-167258d4a7eb, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"outputs_hidden":true},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ae25e3e-e694-4108-8fce-12f2205837dd"},{"cell_type":"code","source":["#Define parameters\n","source_workspace = 'Microsoft Fabric Capacity Metrics'\n","source_dataset = 'Fabric Capacity Metrics'\n","\n","#Specify whether to store raw data in the Files area of the Lakehouse (default is true)\n","store_raw_data = True\n","path_raw = 'Files/Raw Capacity Metrics History/'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"13d3a19c-ed66-4053-a1b7-167258d4a7eb","normalized_state":"finished","queued_time":"2024-11-08T20:54:06.8765422Z","session_start_time":null,"execution_start_time":"2024-11-08T20:54:32.7096447Z","execution_finish_time":"2024-11-08T20:54:32.9690307Z","parent_msg_id":"d0246206-c163-4d05-8a4d-366ba6f1d921"},"text/plain":"StatementMeta(, 13d3a19c-ed66-4053-a1b7-167258d4a7eb, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"eaa2e58b-58e2-441a-8c63-cd451a58b41d"},{"cell_type":"code","source":["%%sql\n","-- Create a table to store the processing log (if it does not already exist)\n","CREATE TABLE IF NOT EXISTS processing_log AS \n","SELECT current_timestamp() AS Timestamp, CAST('2000-01-01' AS date) AS Watermark;"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"f0055aae-b478-430d-9256-f8f63a453772"},{"cell_type":"code","source":["#Get the latest watermark from the processing_log\n","query = \"\"\" SELECT MAX(Watermark) AS Watermark FROM processing_log\"\"\"\n","df = spark.sql(query)\n","watermark = df.first()['Watermark']\n","start_time = datetime.now()\n","start_date = date.today()\n","\n","#Parset date parts\n","start_year = str(start_time.year)\n","start_month = str(start_time.month)\n","start_day = str(start_time.day)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb567199-4606-480b-b602-dcd177f9eac5"},{"cell_type":"markdown","source":["## Extract and Store Item Catalog"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea7e94b5-b451-47da-88e9-5f16611e7faf"},{"cell_type":"code","source":["#Extract the catalog of items from the Fabric Capacity Metrics App semantic model and store as parquet\n","dax_query = \"\"\"\n","    EVALUATE\n","    SUMMARIZECOLUMNS (\n","        Items[Billable type],\n","        Items[capacityId],\n","        Items[dcount_Identity],\n","        Items[IsVirtualArtifactName],\n","        Items[IsVirtualArtifactStatus],\n","        Items[IsVirtualWorkspaceName],\n","        Items[IsVirtualWorkspaceStatus],\n","        Items[ItemId],\n","        Items[ItemKey],\n","        Items[ItemKind],\n","        Items[ItemName],\n","        Items[Timestamp],\n","        Items[UniqueKey],\n","        Items[WorkspaceId],\n","        Items[WorkspaceName]\n","    )\"\"\"\n","\n","df_items = fabric.evaluate_dax(\n","    workspace = source_workspace,\n","    dataset = source_dataset,\n","    dax_string = dax_query,\n","    verbose = 1\n","    )\n","\n","#Convert Fabric DataFrames into Spark DataFrames\n","dfs_items = spark.createDataFrame(df_items)\n","#Clean up column names\n","dfs_items = dfs_items.select([F.col(x).alias(x.replace('Items[','').replace(' ', '_').replace('[', '').replace(']', '').replace('(', '').replace(')','')) for x in dfs_items.columns])\n","#Create a temporary view on top of the data frame\n","dfs_items.createOrReplaceTempView('current_items')\n","\n","if store_raw_data:\n","    #Set path and file name\n","    subfolder = 'Items/' + start_year + '/' + start_month + '/' + start_day + '/'\n","    file_name = start_time.strftime(\"%H-%M-%S\")\n","\n","    #Save DataFrames to OneLake\n","    dfs_items.write.mode(\"overwrite\").format(\"parquet\").save(path_raw + subfolder + file_name)\n","\n","#display(df_items)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8763dd92-23ee-4de9-a57c-eaa6be9f3fcb"},{"cell_type":"code","source":["%%sql\n","-- Create a table to store items catalog (if it does not already exist)\n","CREATE TABLE IF NOT EXISTS items AS \n","SELECT * FROM current_items LIMIT 0; \n","\n","-- Merge currenly retrieved items into the permanent items table\n","MERGE INTO items AS target\n","USING current_items AS source\n","ON source.UniqueKey = target.UniqueKey\n","WHEN MATCHED THEN\n","  UPDATE SET *\n","WHEN NOT MATCHED THEN\n","  INSERT *;"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"3f34e5fc-c03e-4a6f-a5bf-55554cecca5b"},{"cell_type":"markdown","source":["## Extract and Store Hourly Capacity Metrics"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cfbb8ed9-9762-4fb8-9932-06d36f75d2d0"},{"cell_type":"code","source":["#Extract the hourly metrics by item by operation from the Fabric Capacity Metrics App semantic model and store as parquet\n","#The data is pre-filtered from start date forward\n","dax_query = \"\"\"\n","EVALUATE\n","    SUMMARIZECOLUMNS (\n","        'MetricsByItemandOperationandHour'[UniqueKey],\n","        'MetricsByItemandOperationandHour'[ItemId],\n","        'MetricsByItemandOperationandHour'[WorkspaceId],\n","        'MetricsByItemandOperationandHour'[PremiumCapacityId],\n","        'MetricsByItemandOperationandHour'[DateTime],\n","        'MetricsByItemandOperationandHour'[OperationName],\n","        'MetricsByItemandOperationandHour'[sum_CU],\n","        'MetricsByItemandOperationandHour'[sum_duration],\n","        'MetricsByItemandOperationandHour'[Throttling (min)],\n","        'MetricsByItemandOperationandHour'[count_users],\n","        'MetricsByItemandOperationandHour'[count_successful_operations],\n","        'MetricsByItemandOperationandHour'[count_rejected_operations],\n","        'MetricsByItemandOperationandHour'[count_operations],\n","        'MetricsByItemandOperationandHour'[count_Invalid_operations],\n","        'MetricsByItemandOperationandHour'[count_InProgress_operations],\n","        'MetricsByItemandOperationandHour'[count_failure_operations],\n","        'MetricsByItemandOperationandHour'[count_cancelled_operations],\n","        'MetricsByItemandOperationandHour'[avg_DurationMS],\n","        'MetricsByItemandOperationandHour'[percentile_DurationMs_50],\n","        'MetricsByItemandOperationandHour'[percentile_DurationMs_90],\n","    \tFILTER ('MetricsByItemandOperationandHour', \n","            'MetricsByItemandOperationandHour'[DateTime] >= DATE(\"\"\" + start_year + \", \" + start_month + \", \" + start_day + \"\"\")\n","            && ('MetricsByItemandOperationandHour'[sum_CU] > 0 \n","                || 'MetricsByItemandOperationandHour'[sum_duration] > 0 \n","                || 'MetricsByItemandOperationandHour'[count_operations] > 0)\n","            ) \n","        )\n","    \"\"\"\n","\n","df_metrics = fabric.evaluate_dax(\n","    workspace = source_workspace,\n","    dataset = source_dataset,\n","    dax_string = dax_query,\n","    verbose = 1\n","    )\n","         \n","#Convert Fabric DataFrames into Spark DataFrames\n","dfs_metrics = spark.createDataFrame(df_metrics)\n","#Clean up column names\n","dfs_metrics = dfs_metrics.select([F.col(x).alias(x.replace(' ', '_').replace('[', '').replace(']', '').replace('(', '').replace(')','').replace('MetricsByItemandOperationandHour','')) for x in dfs_metrics.columns])\n","#Create a temporary view on top of the data frame\n","dfs_metrics.createOrReplaceTempView('current_metrics')\n","\n","if store_raw_data:\n","    #Set path and file name\n","    subfolder = 'Hourly Metrics/' + start_year + '/' + start_month + '/' + start_day + '/'\n","    file_name = start_time.strftime(\"%H-%M-%S\")\n","    #Save DataFrames to OneLake\n","    dfs_metrics.write.mode(\"overwrite\").format(\"parquet\").save(path_raw + subfolder + file_name)\n","\n","#display(dfs_metrics)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"89c680a5-bb63-4c69-8238-0bf0c408da63"},{"cell_type":"code","source":["%%sql\n","-- Create a table to store hourly capacity metrics (if it does not already exist)\n","CREATE TABLE IF NOT EXISTS hourly_capacity_metrics AS \n","SELECT * FROM current_metrics LIMIT 0; \n","\n","-- Merge currenly retrieved metrics into the permanent table\n","MERGE INTO hourly_capacity_metrics AS target\n","USING current_metrics AS source\n","ON source.UniqueKey = target.UniqueKey\n","    AND source.DateTime = target.DateTime\n","    AND source.OperationName = target.OperationName\n","WHEN MATCHED THEN\n","  UPDATE SET *\n","WHEN NOT MATCHED THEN\n","  INSERT *;"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"35b049b6-c9cb-4d32-a618-437aac9e32e1"},{"cell_type":"markdown","source":["## Log Completion"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"405c2251-b803-4f9b-a159-57382ac90520"},{"cell_type":"code","source":["#Add a record to processing log to indicate completion of a data loading operation\n","query = \"INSERT INTO processing_log (Timestamp, Watermark) SELECT '\" + format(start_date) + \"', '\" + format(start_time) + \"'\"\n","spark.sql(query)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e43b0b35-1089-405e-b375-3e1ff570fe4e"},{"cell_type":"markdown","source":["## Analyze Results\n","\n","Following is a sample query that displays the top 100 items based on resource consumption."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"397c3992-044a-4a59-81ee-658fa17ebc06"},{"cell_type":"code","source":["%%script echo \"Skipping this cell, since it is provided only for illustration purposes and is not needed for data extraction and storage. To execute the sample query shown below, please delete this line.\"\n","%%sql\n","\n","SELECT I.capacityId, I.WorkspaceName, I.ItemKind, I.ItemName, ROUND(SUM(M.sum_CU),2) AS CapacityUnitSecondsConsumed\n","FROM items I \n","    INNER JOIN hourly_capacity_metrics M \n","        ON I.UniqueKey = M.UniqueKey\n","GROUP BY I.capacityId, I.WorkspaceName, I.ItemKind, I.ItemName\n","ORDER BY CapacityUnitSecondsConsumed DESC\n","LIMIT 100"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"f3f901d8-93ed-4f77-9b05-6b6b072ad75a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}