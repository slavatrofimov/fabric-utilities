{"cells":[{"cell_type":"markdown","source":["# Analyze OneLake data volumes in Microsoft Fabric\n","\n","### Purpose and Overview\n","This notebook helps Microsoft Fabric workspace administrators manage and track data stored in OneLake.\n","* Recursively traverse folders and files in a given Workspace or folder path. \n","* Returns a list of all files within  the path broken out by path segment and enrich the data with relevant attributes, such as:\n","    - File Extension\n","    - Modification Time\n","    - Size\n","    - Etc.\n","* Provides a few basic summaries of the collected data: total for the entire path, breakdown by file extension, by Fabric item, by file extension, etc.\n","* Stores a snapshot of collected data (at the granularity of one record per file) into a Lakehouse table for historical record keeping and subsequent analysis.\n","\n","### Usage Instructions\n","This notebook is intended to be executed in a Microsoft Fabric workspace, which is hosted on a Fabric Capacity. The user executing this notebook must have access to the workspace and items whose storage usage is being analyzed (if the current user does not have permissions to a path segment, that segment will be skipped). This notebook can  be used to analyze OneLake storage for any workspace in your Fabric Tenant to which you have sufficient permissions (i.e., this notebook may reside in workspace A while analyzing storage in Workspaces B or C or D).\n","\n","1. Download and import this Spark Notebook into a Fabric Workspace.\n","1. Attach a Default Lakehouse to this notebook. This Lakehouse will be used to store snapshots of data collected by this notebook. This Lakehouse may reside in any Fabric workspace (which may be different from the workspace where this Notebook is located or the workspace whose storage is being analyzed). The person executing this notebook must have full access to this Lakehouse.\n","1. Specify values for the *workspace_name*, *folder_path*, and *snapshot_table_name* parameters in a code cell below.\n","1. Press the \"Run all\" button to run all code cells\n","1. Review the results of summaries provided within the notebook\n","1. If desired, analyze data from the Lakehouse table where snapshots of collected data has been stored. Note that the Lakehouse table includes *RootPath* and *SnapshotTimestamp* columns that can be used to select data for a specific snapshot.\n","\n","If necessary, repeat the process for additional Workspaces or folder paths.\n","\n","### Considerations\n","Collecting a list of files stored in OneLake can be a lengthy process for folder paths with large numbers of files. The duration of the process is influenced by the number of files (not the size of files). This notebook will typically process between hundreds of thousands to a few million files per hour. Due to the way in which storage data collection is parallelized, throughput will be higher when each level of folder hierarchy contains many \"balanced\" folders (with each folder containing similar numbers of files). Throughput will be lower when most files at any level of folder hierarchy are stored in one (or only a few) large folders.\n","\n","### Important Notice!\n","<mark>Data volumes returned by this notebook **will not match data volumes reported by the Fabric Capacity Metrics App or that may appear on your Microsoft Azure bill for your Fabric Capacity** due to substantial differences in methodologies for calculating data volumes.</mark>\n","Following are a few factors that may result in different data data volume calculations:\n","* This notebook will include files available via shortcuts, even if these files do not physically reside in this OneLake path (and are merely logically referenced within this path). \n","* This notebook may include storage that is considered non-billable (e.g., mirroring storage).  \n","* This notebook will skip any paths to which the user running this notebook does not have permissions. \n","* This notebook will not include files that have been deleted, but are being temporarily retained to provide protection from accidental deletion (soft-deleted data in OneLake is billed at the same rate as active data).\n","* This notebook will not capure data for OneLake Cache storage (used by certain items, such as Eventhouses).\n","* The timing of when your storage is calculated will differ from the timing used to calculate billable storage.\n","\n","Therefore, while this notebook can serve as a helpful tool in analyzing your storage, *it should not be used to estimate your billable storage*!\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a784197-c0b0-4b4a-97de-edd75826c314"},{"cell_type":"code","source":["#Specify Fabric workspace name (or leave blank to collect data for the current workspace)\n","workspace_name = \"\"\n","\n","#Specify folder path within the workspace (or leave blank to collect data for all items in the workspace)\n","folder_path = \"\"\n","\n","#Store a snashot of the list of files into the following table in the Default Lakehouse attached to this Notebook\n","snapshot_table_name = \"storage_snapshot\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5aa68225-9d23-448e-b315-7d6d3e1a4497"},{"cell_type":"code","source":["#Import relevant libraries\n","import sempy.fabric as fabric\n","import time\n","\n","#Make sure that the current notebook is attached to a default Lakehouse\n","lh = fabric.get_lakehouse_id()\n","if lh == '' or lh == None:\n","    raise Exception('The Notebook is not attached to a default lakehouse. Please attache a default lakehouse and try again.')\n","\n","#Resolve workspace Id\n","workspace_id = fabric.resolve_workspace_id(None if workspace_name == \"\" else workspace_name)\n","if workspace_id == '' or workspace_id == None:\n","    raise Exception('Workspace name is not valid. Please specify a valid Workspace Name or Workspace name.')\n","\n","path = f\"abfss://\" + str.replace(workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + folder_path + '/', \"//\",\"/\")\n","\n","start_time = time.time()\n","print(\"Processing the following path: \" + path)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"4b385285-5d98-41ed-b232-8b59b21e4bc4"},{"cell_type":"code","source":["import pandas as pd\n","from notebookutils import mssparkutils\n","from typing import List\n","from concurrent.futures import ThreadPoolExecutor\n","import concurrent.futures\n","from pyspark.sql import functions as F\n","\n","def get_descendants(path):\n","    # Recursively get files in a directory path\n","    new_paths = mssparkutils.fs.ls(path)\n","    files = []\n","\n","    def get_children(path):\n","        try:\n","            print(pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" + Processed: \" + path.path)\n","            return mssparkutils.fs.ls(path.path)\n","        except Exception as e:\n","            print(pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" ! Error: \" + path.path + f\". Skipping this path! Error message: {e}\")\n","            return [None]\n","\n","    while len(new_paths) > 0:\n","        current_paths = []\n","        for path in new_paths:\n","            if path.isDir:\n","                current_paths.append(path)\n","        \n","        new_paths = []\n","\n","        #Parallelize retrieval of child items from each path\n","        with ThreadPoolExecutor(max_workers=16) as executor:\n","            results = executor.map(get_children, current_paths)\n","            children = sum(results, [])\n","            for child in children:\n","                if child == None:\n","                    print(pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\") + ' * Ignoring skipped path!')\n","                elif child.isDir:\n","                    new_paths.append(child)\n","                else:\n","                    files.append(child)\n","        print(pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\") + f\" * Files processed : {len(files)}\")\n","    return files\n","\n","def file_list_to_dataframe(files):\n","    if len(files) > 0:\n","        # Convert file list into a Pandas data frame and enrich the data to facilitate analysis\n","        schema = ['path','name','size', 'modifyTime']\n","        df = pd.DataFrame([[getattr(file, attribute) for attribute in schema] for file in files], columns = schema).sort_values('path')\n","        df['FileExtension'] = df['name'].str.split(pat='.', expand=False, n=1).str[0]\n","        df['path'] = df['path'].str.replace(\"abfss://\", \"\", case=False, regex=False)\n","        loc_0 = df.columns.get_loc('path')\n","        df_split = df['path'].str.split(pat='/', expand=True).add_prefix('Path_')\n","        df = pd.concat([df.iloc[:, :loc_0], df_split, df.iloc[:, loc_0:]], axis=1)\n","        df['SizeInMB'] = df['size']/(pow(2, 20))\n","        df['modifyTime'] = pd.to_datetime(df['modifyTime'], unit = 'ms')\n","        df = df.drop(columns=['size'])\n","        df = df.rename(columns={'path': 'FullPath'})\n","        df['Depth'] = df['FullPath'].str.count('/')\n","        return df\n","    else:\n","        print('No files discovered.')\n","        return pd.DataFrame([])\n","        \n","\n","#Get all files from path\n","df = file_list_to_dataframe(get_descendants(path))\n","\n","#Write data to a table\n","if len(df) > 0:\n","    spark_df = spark.createDataFrame(df)\n","    spark_df = spark_df.withColumn(\"RootPath\", F.lit(path)).withColumn('SnapshotTimestamp', F.current_date())\n","    spark_df.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").saveAsTable(snapshot_table_name)\n","    print(pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\") + f\" * Stored a snapshot of the file list into the {snapshot_table_name} table.\")\n","else:\n","    raise Exception(\"No files discovered. Please provide a Workspace and Path containing one or more files.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db6267d8-2873-45dc-80b2-015c64ec6b3b"},{"cell_type":"code","source":["#Summarize data for the entire path\n","dfSummary = df.groupby(['Path_0']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"15d31318-df95-4b38-b365-ad014874a747"},{"cell_type":"code","source":["#Summarize data by Fabric Item\n","dfSummary = df.groupby(['Path_1']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dd292111-5071-49dd-80f9-535786f0f5e5"},{"cell_type":"code","source":["#Summarize data by File Extension\n","dfSummary = df.groupby(['FileExtension']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c7984a06-657c-4a46-a879-53f166f9c376"},{"cell_type":"code","source":["#Summarize data for the first 3 path segments\n","dfSummary = df.groupby(['Path_1', 'Path_2', 'Path_3']).agg(SizeInMB=('SizeInMB', 'sum'), FileCount=('Path_0', 'count'), MeanFileSize=('SizeInMB', 'mean'), MedianFileSize=('SizeInMB', 'median'), MaxFileSize=('SizeInMB', 'max'), LatestModification=('modifyTime', 'max'), MaxDepth=('Depth', 'max')).reset_index()\n","display(dfSummary)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"230df7bf-89b7-467c-b8ec-cffde845220b"},{"cell_type":"code","source":["#Return the full list of files\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"dd99a63e-8f64-4b63-b75e-02b888a18e69"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}